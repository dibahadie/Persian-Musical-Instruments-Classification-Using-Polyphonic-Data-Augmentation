# Persian Musical Instruments Classification Using Polyphonic Data Augmentation

> Repository for the paper **â€œPersian Musical Instruments Classification Using Polyphonic Data Augmentation.â€**

This project introduces a culturally-informed **polyphonic augmentation pipeline** for classifying Persian musical instruments using a fine-tuned **MERT** backbone. The paper and this repository describe dataset creation, augmentation strategies, model design, and evaluation procedures.

---

## ğŸª• Overview

We present a new approach to **Persian instrument classification** using synthetic polyphonic data generated by mixing monophonic recordings that share **dastgÄh** (modal key) and **tempo (BPM)**.

Key highlights:

- Introduces a novel *DastgÄh + BPM* polyphonic augmentation method.
- Uses **MERT-v1-330M** as the pretrained backbone for multi-label classification.
- Achieves:
  - **ROC-AUC:** 0.795 (DastgÄh + BPM)
  - **Accuracy:** 0.841 (DastgÄh-only)
- Evaluated on a manually annotated real-world test set of **491 polyphonic 5-second excerpts**.

---

## ğŸ¼ Dataset

### Dataset on Huggingface
[![Hugging Face Dataset](https://img.shields.io/badge/ğŸ¤—%20Dataset-dibahadie%2FPMID-yellow.svg)](https://huggingface.co/datasets/dibahadie/PMID)

### Summary 

- **Monophonic dataset:** ~16,800 5-second clips across 10 instrument classes  
  *(Ney, Tar, Santur, Kamancheh, Daf, Tonbak, Piano, Violin, Sitar, Avaz)*  
- **Polyphonic (synthetic) dataset:** ~50,000 generated clips  
- **Test set:** 372 manually labeled 5s real-world polyphonic excerpts


---

## ğŸ›ï¸ Data Augmentation Modes

| Mode | Description |
|------|--------------|
| `RAMDOM` | Randomly mixed clips |
| `BPM` | Same tempo, different dastgÄh |
| `DASTGAH` | Same dastgÄh, different tempos |
| `DASTGAH_BPM` | Same dastgÄh *and* tempo (proposed method) |

**Finding:** Tonal alignment (dastgÄh) and tempo alignment both improve classification performance.

---

## ğŸ§  Model

- **Backbone:** `MERT-v1-330M`
- **Classifier:** Multi-label head (weighted layer aggregation + MLP + sigmoid)
- **Loss:** Binary Cross-Entropy with Logits (multi-label)
- **Input:** 5-second audio clips
- **Output:** 10 instrument probabilities

### Architecture Summary

```text
Audio (5s)
   â†“
MERT-v1-330M (pretrained)
   â†“
Weighted layer aggregation
   â†“
Two-layer MLP
   â†“
Sigmoid (multi-label output)
```

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|------------|--------|
| Backbone | `mert-v1-330m` |
| Optimizer | `AdamW` |
| Learning Rate | 1e-4 |
| Batch Size | 16 |
| Epochs | 10 |
| Loss | BCEWithLogitsLoss |

### Train Example


---

## ğŸ“Š Evaluation

### Metrics
- **Accuracy**
- **ROC-AUC**
- **F1-score** (macro & micro)

### Reported Results (from paper)

| Data Augmentation | Accuracy | ROC-AUC | F1-score |
|--------------------|-----------|----------|-----------|
| Random | 0.794 | 0.750 | 0.606 |
| BPM | 0.807 | 0.764 | 0.617 |
| **Dastgah** | **0.841** | 0.780 | **0.669** |
| Dastgah + BPM | 0.823 | **0.795** | 0.652 |

> Evaluation conducted on 491 real-world 5s polyphonic samples.

---

## ğŸ“š Citation

If you use this repository or dataset, please cite:

```
@article{persian_instruments_2025,
  title={Persian Musical Instruments Classification Using Polyphonic Data Augmentation},
  author={...},
  year={2025},
  journal={...}
}
```

---

## ğŸª„ Acknowledgements

- The monophonic dataset was curated from Persian traditional music recordings and annotated by expert musicians.
- Inspired by the rich modal system (*dastgÄh*) of Persian music.
- The pretrained **MERT** model is provided by the Hugging Face team.

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” see the [LICENSE](./LICENSE) file for details.
