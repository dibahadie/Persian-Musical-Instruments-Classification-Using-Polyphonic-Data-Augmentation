# Persian Musical Instruments Classification Using Polyphonic Data Augmentation

> Repository for the paper **“Persian Musical Instruments Classification Using Polyphonic Data Augmentation.”**

This project introduces a culturally-informed **polyphonic augmentation pipeline** for classifying Persian musical instruments using a fine-tuned **MERT** backbone. The paper and this repository describe dataset creation, augmentation strategies, model design, and evaluation procedures.

---

## 🪕 Overview

We present a new approach to **Persian instrument classification** using synthetic polyphonic data generated by mixing monophonic recordings that share **dastgāh** (modal key) and **tempo (BPM)**.

Key highlights:

- Introduces a novel *Dastgāh + BPM* polyphonic augmentation method.
- Uses **MERT-v1-330M** as the pretrained backbone for multi-label classification.
- Achieves:
  - **ROC-AUC:** 0.796 (Dastgāh + BPM)
  - **Accuracy:** 0.838 (Dastgāh-only)
- Evaluated on a manually annotated real-world test set of **372 polyphonic 5-second excerpts**.

---

## 🎼 Dataset

### Summary

- **Monophonic dataset:** ~16,800 5-second clips across 10 instrument classes  
  *(Ney, Tar, Santur, Kamancheh, Daf, Tonbak, Piano, Violin, Sitar, Avaz)*  
- **Polyphonic (synthetic) dataset:** ~50,000 generated clips  
- **Test set:** 372 manually labeled 5s real-world polyphonic excerpts
- 
---

## 🎛️ Data Augmentation Modes

| Mode | Description |
|------|--------------|
| `random` | Randomly mixed clips |
| `bpm` | Same tempo, different dastgāh |
| `dastgah` | Same dastgāh, different tempos |
| `dastgah+bpm` | Same dastgāh *and* tempo (proposed method) |

**Finding:** Tonal alignment (dastgāh) and tempo alignment both improve classification performance.

---

## 🧠 Model

- **Backbone:** `MERT-v1-330M`
- **Classifier:** Multi-label head (weighted layer aggregation + MLP + sigmoid)
- **Loss:** Binary Cross-Entropy with Logits (multi-label)
- **Input:** 5-second audio clips
- **Output:** 10 instrument probabilities

### Architecture Summary

```text
Audio (5s)
   ↓
MERT-v1-330M (pretrained)
   ↓
Weighted layer aggregation
   ↓
Two-layer MLP
   ↓
Sigmoid (multi-label output)
```

---

## ⚙️ Training Configuration

| Parameter | Value |
|------------|--------|
| Backbone | `mert-v1-330m` |
| Optimizer | `AdamW` |
| Learning Rate | 1e-4 |
| Batch Size | 16 |
| Epochs | 10 |
| Loss | BCEWithLogitsLoss |

### Train Example


---

## 📊 Evaluation

### Metrics
- **Accuracy**
- **ROC-AUC**
- **F1-score** (macro & micro)

### Reported Results (from paper)

| Augmentation | Accuracy | ROC-AUC |
|---------------|-----------|----------|
| Random | 0.769 | 0.755 |
| BPM-only | 0.801 | 0.785 |
| Dastgāh-only | **0.838** | 0.776 |
| Dastgāh + BPM | 0.817 | **0.796** |

> Evaluation conducted on 372 real-world 5s polyphonic samples.

---

## 📚 Citation

If you use this repository or dataset, please cite:

```
@article{persian_instruments_2025,
  title={Persian Musical Instruments Classification Using Polyphonic Data Augmentation},
  author={...},
  year={2025},
  journal={...}
}
```

---

## 🪄 Acknowledgements

- The monophonic dataset was curated from Persian traditional music recordings and annotated by expert musicians.
- Inspired by the rich modal system (*dastgāh*) of Persian music.
- The pretrained **MERT** model is provided by the Hugging Face team.

---

## 📜 License

This project is licensed under the **MIT License** — see the [LICENSE](./LICENSE) file for details.
